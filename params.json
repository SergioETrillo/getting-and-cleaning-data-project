{"name":"Getting-and-cleaning-data-project","tagline":"Course Project getting and cleaning data","body":"# getting-and-cleaning-data-project\r\nCourse Project getting and cleaning data\r\nThis project uses data from the project \r\nAll the data is available at\r\nhttps://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip \r\n\r\nThe dataset from this project comes from:\r\n\r\n[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012\r\n\r\nThe project requirements are available at:\r\n\r\nhttps://class.coursera.org/getdata-013/human_grading/view/courses/973500/assessments/3/submissions\r\n\r\nOutput:\r\n\r\nIn the repository https://github.com/tigretoncio/getting-and-cleaning-data-project there are 3 files available as solution for this project:\r\n\r\nReadme.md: (this file)\r\nCodebook.md : Contains the code book information of the variables used\r\nrun_analysis.R: The script code that cleans the data and performs the required activities\r\n\r\n# How to run the script run_analysis.R\r\nUnzip the data of the project, and copy the file run_analysis.R in the \"UCI HAR Dataset\" folder. \r\nSet the working directory on this \"UCI HAR Dataset\" folder too.\r\nExecute the script using source(\"run_analysis.R\").\r\n\r\nThe script will create the tidy data output \"av_data.txt\".  That result was added to the submission page.  \r\n\r\nrun_analysis.R uses the package dplyr. If not installed you can use the command > install.packages(\"dplyr\") if the package is not available.\r\n\r\n# To read the output in \"av_data.txt\"\r\n\r\nThe way of reading the file with tidy output is as follows:\r\n\r\n  data <- read.table(file_path, header = TRUE) \r\n  View(data)\r\n\r\n(as per https://class.coursera.org/getdata-013/forum/thread?thread_id=30)\r\n\r\nPlease note that if open in notepad there wonÂ´t be a clean way of viewing all data.\r\n\r\nThis output is a table of 180 rows and 68 columns.  180 rows because there are 30 individuals performing 6 different activities.  68 columns because they are 66 the variables related with mean and standard deviation as required in the project, plus 2 more columns for the activity and the subject ID.\r\n\r\n# Structure of the input data\r\n\r\nThe data is being provided in different folders and files, mainly there are two sets of data: test and train.  The result comes from the original data and performing the following cleaning up activities as per the project requirements.  The activities that the 30 individuals perform are found in file \"activity_labels.txt\" with the following codes:\r\n\r\n1\tWALKING\r\n2\tWALKING_UPSTAIRS\r\n3\tWALKING_DOWNSTAIRS\r\n4\tSITTING\r\n5\tSTANDING\r\n6\tLAYING\r\n\r\nThe variables measured are available in X_train and X_test, and contain raw data of 561 variables.  Script cleans data to reduce variable to 68 columns, which are the only ones with mean() and std() elements. Other variables such as gravityMean have been conciously left out of the solution, because the variable is not an explicit mean calculation.  This strategic decision is supported by https://class.coursera.org/getdata-013/forum/thread?thread_id=30\r\n\r\n\r\n\r\nThe script performs the following activities:\r\n## 0.- Reads all the data files\r\n\r\n## 1.- Merges the training and the test sets to create one data set.\r\nFor that it adds columns Activity and Subject ID to the train and test datasets, and then merges both datasets together using rbind, removing from memory temporary elements not used anymore\r\n\r\n## 2.- Extracts only the measurements on the mean and standard deviation for each measurement. \r\nAs above, 68 columns are extracted.  See Codebook for further information, and \"features_info.txt\" in the \"UCI HAR Dataset\" folder for further info about these variables.\r\n## 3.- Uses descriptive activity names to name the activities in the data set (68 columns)\r\nChanges the codes available in \"activity_labels.txt\" into the descriptive name.\r\n\r\n\r\n## 4.- Appropriately labels the data set with descriptive variable names. \r\nThe following transformations have been applied:\r\n* temp <- gsub(\"^t\",\"time-\",temp)   : A variable that starts with t is changed to start with \"time\"  \r\n* temp <- gsub(\"^f\",\"frequency-\",temp) A variable that starts with f is changed to start with \"frequency\"  \r\n* temp <- gsub(\"Acc\",\"_Acceleration\",temp) If variable contains \"Acc\" it is changed to \"Acceleration\"  \r\n* temp <- gsub(\"Gyro\",\"_Gyroscope-\",temp) If variable contains \"Gyro\" it is changed to \"Gyroscope\"  \r\n* temp <- gsub(\"Mag\",\"-Magnitude-\",temp) If variable contains \"Mag\" it is changed to \"Magnitude\"  \r\n* temp <- gsub(\"BodyBody\",\"Body-\",temp) This is to correct evident errors in data: there are variables containing \"BodyBody\" and are changed to \"Body\"  \r\n* temp <- gsub(\"std\",\"standard_deviation\",temp)  If variable contains \"std\" it is changed to \"standard_deviation\"  \r\n* temp <- gsub(\"--\",\"-\",temp) Those ones are to uniformize variable names  \r\n* temp <- gsub(\"-_\",\"_\",temp) Some as above  \r\n\r\n## 5.- From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.\r\n\r\nThe result of the script analysis is a text document of name \"av_data.txt\". This contains a table of 180 rows and 68 columns, 180 is the result of having 30 individuals performing 6 different activities, and having collected 68 different variables per individual and activity.  This respects the tidy structrure required for the project.  This is performed using the dplyr package grouping by Activity and Subject_ID and summarising the data.\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}